<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="36-350 – Statistical Computing" />
  <meta name="font-size-adjustment" content="-1"/>
  <title>Optimization</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ff0000; font-weight: bold; } /* Alert */
            code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #7d9029; } /* Attribute */
            code span.bn { color: #40a070; } /* BaseN */
            code span.bu { } /* BuiltIn */
            code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4070a0; } /* Char */
            code span.cn { color: #880000; } /* Constant */
            code span.co { color: #60a0b0; font-style: italic; } /* Comment */
            code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #ba2121; font-style: italic; } /* Documentation */
            code span.dt { color: #902000; } /* DataType */
            code span.dv { color: #40a070; } /* DecVal */
            code span.er { color: #ff0000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #40a070; } /* Float */
            code span.fu { color: #06287e; } /* Function */
            code span.im { } /* Import */
            code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #007020; font-weight: bold; } /* Keyword */
            code span.op { color: #666666; } /* Operator */
            code span.ot { color: #007020; } /* Other */
            code span.pp { color: #bc7a00; } /* Preprocessor */
            code span.sc { color: #4070a0; } /* SpecialChar */
            code span.ss { color: #bb6688; } /* SpecialString */
            code span.st { color: #4070a0; } /* String */
            code span.va { color: #19177c; } /* Variable */
            code span.vs { color: #4070a0; } /* VerbatimString */
            code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
          </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="Notes_7B_files/header-attrs-2.11/header-attrs.js"></script>
  <link href="Notes_7B_files/slidy-2/styles/slidy.css" rel="stylesheet" />
  <script src="Notes_7B_files/slidy-2/scripts/slidy.js"></script>
  <script src="Notes_7B_files/slidy_shiny-1/slidy_shiny.js"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Optimization</h1>
  <p class="author">
36-350 – Statistical Computing
  </p>
  <p class="date">Week 7 – Spring 2022</p>
</div>
<div id="motivation" class="slide section level1">
<h1>Motivation</h1>
<p>Let’s say you’ve a dataset like this one:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(my.data,<span class="at">xlab=</span><span class="cn">NULL</span>,<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fu">max</span>(my.data)))</span></code></pre></div>
<p><img src="Notes_7B_files/figure-slidy/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" />
Two questions naturally arise:</p>
<ul>
<li><p>from what family of distributions were these data sampled?
(normal? Poisson?)</p></li>
<li><p>what are the values of the parameters for that family of
distributions? (what are the true values of <span
class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma\)</span>, conditional on the normal
distribution being the correct one?)</p></li>
</ul>
<p>Optimization is the action of answering the second question, and
providing information that can help you answer the first one. (Note that
in fitting models, we don’t just fit distributions to data, but we’ll
start there…)</p>
<p>Optimization is a <strong>broad</strong> topic, <a
href="https://cran.r-project.org/web/views/Optimization.html">as
indicated here</a>. In this set of slides I will point you towards basic
workhorse optimization functions.</p>
</div>
<div id="optimization-an-intuitive-picture"
class="slide section level1">
<h1>Optimization: an Intuitive Picture</h1>
<p>In most basic terms, optimization is the action of taking a function
<span class="math inline">\(y = f(x \vert \theta)\)</span>, where <span
class="math inline">\(x\)</span> are, e.g., observed data and <span
class="math inline">\(\theta\)</span> are, e.g., distribution
parameter(s), and (numerically) finding the value(s) of <span
class="math inline">\(\theta\)</span> that minimize(s) <span
class="math inline">\(y\)</span>.</p>
<p>In statistics, a common example of <span
class="math inline">\(f(\cdot)\)</span> is the negative log-likelihood
function. Let’s assume we are dealing with a Poisson distribution. In
this context, <span class="math inline">\(\theta = \lambda\)</span> and
<span class="math inline">\(y = f(x \vert \theta) \propto - \sum_i (x_i
\log \lambda - \lambda)\)</span>. We want to find <span
class="math inline">\(\lambda\)</span> such that <span
class="math inline">\(y\)</span> is minimized. You learned (right?) how
to do this analytically in 36-226.</p>
<p>Here we show how to do this numerically, since the <em>vast majority
of optimization problems in statistics can only be approached
numerically</em>. If you examine all the distributions that are listed
in, e.g., Wikipedia, you’d be surprised by how few there are for which
you can perform maximum likelihood estimation with pencil and paper.
(Those are the ones you get tested on in 36-226!)</p>
</div>
<div id="optimization-an-intuitive-picture-1"
class="slide section level1">
<h1>Optimization: an Intuitive Picture</h1>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>f <span class="ot">=</span> <span class="cf">function</span>(x,lambda) {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span><span class="fu">sum</span>(x<span class="sc">*</span><span class="fu">log</span>(lambda)<span class="sc">-</span>lambda))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>lambda.test <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">5</span>,<span class="dv">11</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>neg.log.like <span class="ot">=</span> <span class="fu">rep</span>(<span class="cn">NA</span>,<span class="fu">length</span>(lambda.test))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="fu">seq_along</span>(lambda.test) ) {</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  neg.log.like[ii] <span class="ot">=</span> <span class="fu">f</span>(my.data,lambda.test[ii])</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lambda.test,neg.log.like,<span class="at">typ=</span><span class="st">&quot;l&quot;</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fl">8.13</span>,<span class="at">col=</span><span class="st">&quot;firebrick1&quot;</span>)</span></code></pre></div>
<p><img src="Notes_7B_files/figure-slidy/unnamed-chunk-4-1.png" width="768" /></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;lambda.hat = &quot;</span>,lambda.test[<span class="fu">which.min</span>(neg.log.like)],<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## lambda.hat =  8.13</code></pre>
</div>
<div id="optimization-local-vs.-global" class="slide section level1">
<h1>Optimization: Local vs. Global</h1>
<p>Not all lines or surfaces to be optimized over behave simply:</p>
<center>
<img src="http://www.stat.cmu.edu/~mfarag/350/global.png" width="600" />
</center>
</div>
<div id="optimization-local-vs.-global-1" class="slide section level1">
<h1>Optimization: Local vs. Global</h1>
<p>Local:</p>
<ul>
<li><p>Start at a point in parameter space (<span
class="math inline">\(\hat{\mu}\)</span>,<span
class="math inline">\(\hat{\sigma}\)</span>) and compute the fit metric
at locations “adjacent to” that point. For one of these locations, the
fit metric will be minimized. Shift your estimate (<span
class="math inline">\(\hat{\mu}\)</span>,<span
class="math inline">\(\hat{\sigma}\)</span>) to that location (if you
don’t estimate the derivative of the fit metric) or in that direction
(if you do). Iterate until you cannot move anymore.</p></li>
<li><p>Advantage: fast. Disadvantage: how do you know you started in a
good location in parameter space? The fit metric minimum you reach might
not be the global minimum…it might just be a local one and you wouldn’t
necessarily know it.</p></li>
</ul>
<p>Global:</p>
<ul>
<li><p>Start at a point in parameter space (<span
class="math inline">\(\hat{\mu}\)</span>,<span
class="math inline">\(\hat{\sigma}\)</span>) and compute the fit metric
at locations “adjacent to” that point, or on a grid of values, etc.
Don’t necessarily move towards minima, and search over the whole
parameter space (as well as you are able to). Iterate until you’ve
decided you’ve had enough, and adopt the value (<span
class="math inline">\(\hat{\mu}\)</span>,<span
class="math inline">\(\hat{\sigma}\)</span>) for which the fit metric
was minimized.</p></li>
<li><p>Advantage: will <em>maybe</em> eventually find the global fit
metric minimum. Disadvantage: slow.</p></li>
</ul>
</div>
<div id="local-optimization-optim" class="slide section level1">
<h1>Local Optimization: optim()</h1>
<p>Usage: <code>optim(par,fn,...)</code></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>my.data <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">30</span>,<span class="at">mean=</span><span class="dv">4</span>,<span class="at">sd=</span><span class="fl">1.25</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Return negative log-likelihood for normal distribution.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># par[1] = mu  par[2] = sigma</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>my.fit.fun <span class="ot">=</span> <span class="cf">function</span>(my.par,my.data)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">dnorm</span>(my.data,<span class="at">mean=</span>my.par[<span class="dv">1</span>],<span class="at">sd=</span>my.par[<span class="dv">2</span>])))                    <span class="co"># See Lab 07 for a derivation.</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>my.par <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>) <span class="co"># initial guesses for mu and sigma</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>optim.out <span class="ot">=</span> <span class="fu">optim</span>(my.par,my.fit.fun,<span class="at">my.data=</span>my.data,<span class="at">method=</span><span class="st">&quot;Nelder-Mead&quot;</span>)  <span class="co"># don&#39;t compute gradient</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>optim.out<span class="sc">$</span>value <span class="co"># the minimum of -log.likelihood</span></span></code></pre></div>
<pre><code>## [1] 43.36258</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>optim.out<span class="sc">$</span>par   <span class="co"># true values are 4 and 1.25</span></span></code></pre></div>
<pre><code>## [1] 3.896247 1.026945</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(my.data,<span class="at">prob=</span><span class="cn">TRUE</span>,<span class="at">main=</span><span class="cn">NULL</span>,<span class="at">xlab=</span><span class="cn">NULL</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.4</span>))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span>(<span class="fu">min</span>(my.data),<span class="fu">max</span>(my.data),<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,<span class="fu">dnorm</span>(x,<span class="at">mean=</span>optim.out<span class="sc">$</span>par[<span class="dv">1</span>],<span class="at">sd=</span>optim.out<span class="sc">$</span>par[<span class="dv">2</span>]))</span></code></pre></div>
<p><img src="Notes_7B_files/figure-slidy/unnamed-chunk-6-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>See also: <code>nlm()</code></p>
</div>
<div id="local-optimization-optim-1" class="slide section level1">
<h1>Local Optimization: optim()</h1>
<p>Here we add a function that computes the gradient of the fit metric,
for more efficient optimization.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>my.fit.gradient <span class="ot">=</span> <span class="cf">function</span>(my.par,my.data)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># requires taking partial derivatives of the fit metric with respect to mu (first element of output)</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#   and sigma (second element of output)</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">sum</span>(my.par[<span class="dv">1</span>]<span class="sc">-</span>my.data)<span class="sc">/</span>my.par[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>,<span class="sc">-</span><span class="fu">sum</span>((my.data<span class="sc">-</span>my.par[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>my.par[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">3</span><span class="sc">+</span><span class="fu">length</span>(my.data)<span class="sc">/</span>my.par[<span class="dv">2</span>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>my.par <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">1</span>) <span class="co"># initial guesses for mu and sigma</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># compute gradient</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>optim.out <span class="ot">=</span> <span class="fu">suppressWarnings</span>(<span class="fu">optim</span>(my.par,my.fit.fun,<span class="at">gr=</span>my.fit.gradient,<span class="at">my.data=</span>my.data,<span class="at">method=</span><span class="st">&quot;BFGS&quot;</span>))  </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>optim.out<span class="sc">$</span>value <span class="co"># the minimum of -log.likelihood</span></span></code></pre></div>
<pre><code>## [1] 43.36258</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>optim.out<span class="sc">$</span>par   <span class="co"># true values are 4 and 1.25</span></span></code></pre></div>
<pre><code>## [1] 3.896405 1.026834</code></pre>
</div>
<div id="local-one-parameter-optimization-optimize"
class="slide section level1">
<h1>Local One-Parameter Optimization: optimize()</h1>
<p>Usage: <code>optimize(f,interval,...)</code></p>
<p>“The function <code>optimize</code> searches the interval…for a
minimum…of the function <code>f</code> with respect to its first
argument.”</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Return negative log-likelihood for normal distribution with fixed mean.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>my.fit.fun.sd <span class="ot">=</span> <span class="cf">function</span>(sigma,my.data,my.mu)  <span class="co"># R assumes first argument (sigma) lies within interval</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">dnorm</span>(my.data,<span class="at">mean=</span>my.mu,<span class="at">sd=</span>sigma)))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># search for the optimum value of sigma between 0.1 and 5, conditional on mu being 4</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>optim.out <span class="ot">=</span> <span class="fu">optimize</span>(my.fit.fun.sd,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.1</span>,<span class="dv">5</span>),<span class="at">my.data=</span>my.data,<span class="at">my.mu=</span><span class="dv">4</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>optim.out <span class="co"># true value 1.25</span></span></code></pre></div>
<pre><code>## $minimum
## [1] 1.032043
## 
## $objective
## [1] 43.51448</code></pre>
</div>
<div id="local-linearly-constrained-optimization-constroptim"
class="slide section level1">
<h1>Local (Linearly) Constrained Optimization: constrOptim()</h1>
<p>Usage: <code>constrOptim(theta,f,grad,ui,ci,...)</code></p>
<p>As a purely academic exercise, let’s suppose we wish to put a linear
constraint upon the fit, like <span class="math inline">\(\mu + \sigma
\geq 6\)</span>. We can do that here.</p>
<p>The constraint(s) are specified as <code>ui %*% theta - ci &gt;=
0</code>. <code>theta</code> is your parameter vector (here, the column
vector with <span class="math inline">\(\mu\)</span> above and <span
class="math inline">\(\sigma\)</span> below). For our particular
example, <code>ui</code> is the 1 <span
class="math inline">\(\times\)</span> 2 matrix (1,1) and <code>ci</code>
is 6. (Write it out…once you do the math, you will see that we get the
constraint we defined above.)</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>my.par <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="fl">1.5</span>) <span class="co"># initial guesses for mu and sigma -- must be in feasible region!</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>ui <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>),<span class="at">nrow=</span><span class="dv">1</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>ci <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>optim.out <span class="ot">=</span> <span class="fu">constrOptim</span>(my.par,my.fit.fun,<span class="at">grad=</span><span class="cn">NULL</span>,<span class="at">ui=</span>ui,<span class="at">ci=</span>ci,<span class="at">my.data=</span>my.data)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>optim.out<span class="sc">$</span>value     <span class="co"># the minimum of -log.likelihood</span></span></code></pre></div>
<pre><code>## [1] 49.1855</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>optim.out<span class="sc">$</span>par       <span class="co"># true values are 4 and 1.25</span></span></code></pre></div>
<pre><code>## [1] 4.485561 1.514439</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(optim.out<span class="sc">$</span>par)  <span class="co"># on the boundary</span></span></code></pre></div>
<pre><code>## [1] 6</code></pre>
<p>Note that this is related to methods of <em>convex optimization</em>,
which are beyond the scope of this class. For more details on convex
optimization using <code>R</code>, see <a
href="http://www.stat.cmu.edu/~mfarag/350/ConvexOpt.pdf">this 2014 paper
in the Journal of Statistical Software</a> by Roger Koenker and Ivan
Mizera.</p>
</div>
<div id="global-optimization-a-plethora-of-options"
class="slide section level1">
<h1>Global Optimization: a Plethora of Options</h1>
<p>For an overview of global optimization methods in <code>R</code>, see
<a href="http://www.stat.cmu.edu/~mfarag/350/GlobalOpt.pdf">this 2014
paper in the Journal of Statistical Software</a> by Katherine
Mullen.</p>
<p>Some terms to use in your Google searches if you are looking to try
global optimization:</p>
<ul>
<li><p>“simulated annealing”…this algorithm can be applied in
<code>optim()</code> if you set <code>method="SANN"</code></p></li>
<li><p>“genetic algorithms” (e.g., as implemented in the <code>GA</code>
package), or “evolutionary algorithms”</p></li>
<li><p>“particle swarm”</p></li>
<li><p>“branch and bound” (but note that such an algorithm is most often
used for <em>combinatorial optimization</em>, which is finding the
optimum object from a finite set of objects…what we cover in this set of
slides is continuous optimization)</p></li>
</ul>
</div>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>
</html>
